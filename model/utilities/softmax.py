import torch
import torch.nn as nn


def softmax(x, tau):
    maxes = torch.max(x, 1, keepdim=True)[0]
    x_exp = torch.exp((x-maxes) / tau)
    x_exp_sum = torch.sum(x_exp, 1, keepdim=True)
    output = x_exp/x_exp_sum
    return output


if __name__ == '__main__':
    x = torch.randn(1, 200)
    # output = F.softmax(x, 1)
    output = nn.Softmax(dim=1)(x)
    output_custom = softmax(x, tau=1)

    print(torch.allclose(output, output_custom))
    print(torch.sum(torch.abs(output-output_custom)))

    x = torch.tensor([[-2160.1782, -1465.6573, -1716.5631, -1860.2570, -1707.0658, -1315.3038,
            -1616.6621, -1452.6652, -1781.8800, -1545.8232, -1660.9115, -1696.0791,
            -1664.2003, -1378.9027, -912.2169, -1503.8511, -1417.7589, -1793.6552,
            -1333.3153, -1886.3353, -1365.9016, -1323.5668, -1442.4418, -1268.6644,
            -1383.9769, -1744.9657, -1782.1586, -1638.8931, -1425.7133, -1943.1201,
            -1634.4122, -1502.8784, -1701.2982, -1887.2662, -1508.9586, -1394.0687,
            -1869.1068, -1691.5197, -1603.8234, -1865.7195, -1807.1808, -1517.6221,
            -1241.0216, -1246.5017, -1291.3196, -1907.3560, -1572.6273, -1423.4015,
            -1672.2349, -1394.6221, -1831.9280, -1809.6200, -1749.9441, -1590.8434,
            -1921.8566, -1421.7444, -1804.2543, -1669.3782, -1771.5514, -1852.4353,
            -1635.8256, -1749.9385, -1850.6699, -1434.0051, -1651.0977, -1953.8530,
            -1718.4979, -1643.7876, -1701.0321, -1678.7983, -1345.2312, -1795.0664,
            -1585.8827, -1537.5703, -1777.4028, -1847.0524, -1702.5031, -1502.8246,
            -1876.7010, -1465.9906, -1651.4369, -1452.9243, -1685.9937, -1912.0234,
            -1407.1056, -1964.1261, -1324.6654, -1558.1754, -1545.8041, -1460.6083,
            -1681.2421, -1432.1431, -1784.9364, -1671.8923, -1362.5133, -1517.6191,
            -1666.9189, -1991.0308, -1544.4696, -2371.3542, -1698.2726, -1278.0529,
            -1601.9089, -1470.9999, -1452.5564, -1697.4166, -1607.3843, -1543.0649,
            -1311.4111, -1299.0620, -1520.0947, -1314.4846, -1590.0118, -1576.2510,
            -1350.8971, -1702.2943, -2022.9500, -1429.9458, -2085.5764, -1375.7361,
            -1832.3711, -1569.8870, -1434.2145, -1527.3788, -1423.4407, -1475.3058,
            -1498.1537, -1262.7640, -1382.8812, -1302.6954, -1961.8379, -1540.1232,
            -1781.7744, -1718.3899, -1724.5007, -1296.6443, -1678.7675, -1712.8024,
            -1752.2551, -1451.4966, -2238.7900, -1434.4197, -1942.3687, -1679.6539,
            -1789.9220, -1894.2324, -1259.9652, -1480.3883, -1213.4974, -1467.3807,
            -1560.9962, -1600.2275, -1707.6674, -1593.3182, -1357.0259, -1650.2017,
            -1841.6373, -1498.0024, -1870.9990, -1801.5713, -1561.6315, -1381.4365,
            -1318.9917, -1877.0483, -1697.4277, -1767.3475, -1534.1022, -1734.5236,
            -1302.3352, -1460.4476, -1493.8873, -1251.4023, -1384.1719, -1621.3772,
            -1258.7069, -1715.8118, -1854.1127, -1126.2559, -1700.2493, -1659.8560,
            -1658.7665, -1446.5482, -2027.8837, -1780.6025, -1861.6958, -1621.0536,
            -1634.4813, -1929.4180, -1798.2338, -1794.8917, -1592.7161, -1462.3053,
            -1675.0496, -2066.5706, -1564.2183, -1412.1837, -1503.8605, -1827.6852,
            -1808.5177, -1792.2362]])

    output = nn.Softmax(dim=1)(x)
    print(output)
    output_custom = softmax(x, tau=1)
    print(torch.allclose(output, output_custom))
    output_custom = softmax(x, tau=100)
    print(output_custom)